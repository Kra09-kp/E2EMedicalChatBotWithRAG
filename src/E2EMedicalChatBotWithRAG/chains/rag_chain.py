from src.E2EMedicalChatBotWithRAG.logger import logger
from src.E2EMedicalChatBotWithRAG.models.llm_model import LLMAssistant
from src.E2EMedicalChatBotWithRAG.vector_database import RedisDB, PineconeDB
from src.E2EMedicalChatBotWithRAG.exceptions import AppException
from langchain.schema.runnable import RunnablePassthrough

class RAGChain:
    def __init__(self,use_redis:bool = False):
        self.llm_assistant = LLMAssistant()
        if use_redis:
            self.vector_store = RedisDB()
        else:
            self.vector_store = PineconeDB()

        self.chain = self._create_chain()

    def invoke(self,question: str):
        """
            Synchronously call the RAG chain with the given question.

            Args:
                question (str): The input question to be answered.

            Returns:
                str: The answer generated by the RAG chain.

            Raises:
                AppException: If there is an error during the chain execution.
        """
        try:
            for token in self.chain.stream(question):
                yield token.content
   
        except Exception as e:
            raise AppException(e) from e

    def _create_chain(self):
        """
            Build and return a Retrieval-Augmented Generation (RAG) chain.

            This wires:
                • An LLM and prompt from LLMAssistant
                • A Pinecone retriever from PineconeDB (must already have an index)

            Returns:
                langchain.schema.runnable.Runnable: A runnable RAG chain.

            Raises:
                AppException: If the index is missing or the chain cannot be created.
        """
        try:
            llm = self.llm_assistant.get_model()
            prompt = self.llm_assistant.get_template()
            retriever = self.vector_store.get_retriever()
            rag_chain = (
            {
                "context": retriever,          # Pinecone search → returns docs
                "input": RunnablePassthrough()  # pass the raw user query
            }
            | prompt
            | llm  
        )

            return rag_chain
        except Exception as e:
            logger.error(f"Error in creating RAG chain: {e}")
            raise AppException(e) from e
    