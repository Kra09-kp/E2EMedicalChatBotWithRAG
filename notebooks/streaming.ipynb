{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4cb3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/kirti/Dev/GenAI/E2EMedicalChatBotWithRAG/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5316c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/kirti/Dev/GenAI/E2EMedicalChatBotWithRAG\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e912e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:56:39,462|(INFO)| File: helper | Message: Loaded environment variable: GROQ_API_KEY]\n"
     ]
    }
   ],
   "source": [
    "from E2EMedicalChatBotWithRAG.models.llm_model import LLMAssistant\n",
    "llm_class = LLMAssistant()\n",
    "llm = llm_class.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6193207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b038d8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:56:46,226|(INFO)| File: helper | Message: Loaded environment variable: PINECONE_API_KEY]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/kirti/Dev/GenAI/E2EMedicalChatBotWithRAG/.venv/lib/python3.12/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:56:54,572|(INFO)| File: SentenceTransformer | Message: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2]\n",
      "[2025-09-23 16:57:07,397|(INFO)| File: embedding_model | Message: Successfully loaded embedding model: sentence-transformers/all-MiniLM-L6-v2]\n"
     ]
    }
   ],
   "source": [
    "from src.E2EMedicalChatBotWithRAG.vector_database.pinecone_db import PineconeDB\n",
    "sync_retriever = PineconeDB().get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906bd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.pinecone_asyncio import PineconeAsyncio \n",
    "from src.E2EMedicalChatBotWithRAG.utils.helper import load_env_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be14833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:57:16,286|(INFO)| File: helper | Message: Loaded environment variable: PINECONE_API_KEY]\n"
     ]
    }
   ],
   "source": [
    "pinecone_api_key = load_env_variable(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d32a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PineconeAsyncio(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55ffa46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import PineconeAsyncio\n",
    "async def get_list_of_indexes():\n",
    "    async with PineconeAsyncio(api_key=pinecone_api_key) as pc:\n",
    "        # Do async things\n",
    "        index_list = await pc.list_indexes()\n",
    "        return index_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6afe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_indexes = await get_list_of_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77b5ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"medical-chatbot\",\n",
       "        \"metric\": \"cosine\",\n",
       "        \"host\": \"medical-chatbot-60n3d5q.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 384,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be39f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_index(host_name, pinecone_api_key):\n",
    "    \"\"\"\n",
    "    You can find host name by calling get_list_of_indexes() function\n",
    "    \"\"\"\n",
    "    async with PineconeAsyncio(api_key=pinecone_api_key) as pc:\n",
    "        index = pc.IndexAsyncio(host=host_name)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc1af000",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_index = await get_index(host_name=\"medical-chatbot-60n3d5q.svc.aped-4627-b74a.pinecone.io\", pinecone_api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f56d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 17:02:48,689|(INFO)| File: SentenceTransformer | Message: Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2]\n",
      "[2025-09-23 17:02:55,747|(INFO)| File: embedding_model | Message: Successfully loaded embedding model: sentence-transformers/all-MiniLM-L6-v2]\n"
     ]
    }
   ],
   "source": [
    "from src.E2EMedicalChatBotWithRAG.models.embedding_model import EmbeddingModel\n",
    "embedding_model = EmbeddingModel()._get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4487cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncPineconeRetriever:\n",
    "    def __init__(self, index, embedding_model, k=3):\n",
    "        self.index = index\n",
    "        self.embedding_model = embedding_model\n",
    "        self.k = k\n",
    "\n",
    "    async def get_relevant_docs(self, query_text):\n",
    "        # 1. Get embedding of the query\n",
    "        query_vector = self.embedding_model.embed_query(query_text)\n",
    "\n",
    "        # 2. Query Pinecone\n",
    "        response = await self.index.query(vector=query_vector,\n",
    "                                         top_k=self.k,\n",
    "                                        include_metadata=True)\n",
    "        docs = []\n",
    "        for match in response['matches']:\n",
    "            docs.append(Document(\n",
    "                    # id=match[\"id\"],\n",
    "                    page_content=match['metadata']['text'],\n",
    "                    metadata={\"source\": match['metadata']['source'],\n",
    "                              \"similarity_score\": match['score']}\n",
    "                ))\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e70d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = AsyncPineconeRetriever(index=async_index, embedding_model=embedding_model, k=3)\n",
    "# docs = await retriever.get_relevant_docs(\"what is acne\")\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46d610d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import AsyncCallbackManagerForRetrieverRun, CallbackManagerForRetrieverRun\n",
    "from langchain.schema import BaseRetriever, Document\n",
    "from typing import Any, Optional\n",
    "from pydantic import PrivateAttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ec6b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangChainAsyncRetriever(BaseRetriever):\n",
    "    # Make async_retriever private to avoid Pydantic validation issues\n",
    "    _async_retriever: Any = PrivateAttr()\n",
    "    \n",
    "    k: int = 3\n",
    "    search_kwargs: dict = {}\n",
    "    tags: Optional[list[str]] = None\n",
    "\n",
    "    def __init__(self, async_retriever: AsyncPineconeRetriever, k: int = 3):\n",
    "        super().__init__()\n",
    "        self._async_retriever = async_retriever\n",
    "        self.k = k\n",
    "        self.search_kwargs = {\"k\": k}\n",
    "        self.tags = [\"PineconeVectorStore\", \"HuggingFaceEmbeddings\"]\n",
    "\n",
    "    # We can skip implementing the sync version if you only plan async usage\n",
    "    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> list[Document]:\n",
    "        raise NotImplementedError(\"Use async version only\")\n",
    "\n",
    "    async def _aget_relevant_documents(self, query: str, *, run_manager: AsyncCallbackManagerForRetrieverRun) -> list[Document]:\n",
    "        docs = await self._async_retriever.get_relevant_docs(query)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ff6f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pinecone async wrapper\n",
    "pinecone_retriever = AsyncPineconeRetriever(index=async_index, embedding_model=embedding_model)\n",
    "\n",
    "# Wrap it in LangChain async retriever\n",
    "retriever = LangChainAsyncRetriever(async_retriever=pinecone_retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d8c2c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_response = await retriever.ainvoke(\"who build this project?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc0a675d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'kirti pogra', 'similarity_score': 0.363729477}, page_content='This project is built by Kirti Pogra using LangChain, Pinecone and Groq. The code is available on GitHub. This project is only for educational purposes.'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf', 'similarity_score': 0.267733604}, page_content='ty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-\\ntion, agency, institution, publication, service, or individual does not\\nimply endorsement of the editor or publisher. Errors brought to the\\nattention of the publisher and verified to the satisfaction of the publish-\\ner will be corrected in future editions.\\nThis book is printed on recycled paper that meets Environmental Pro-\\ntection Agency standards.\\nThe paper used in this publication meets the minimum requirements of\\nAmerican National Standard for Information Sciences-Permanence\\nPaper for Printed Library Materials, ANSI Z39.48-1984.\\nThis publication is a creative work fully protected by all applicable\\ncopyright laws, as well as by misappropriation, trade secret, unfair com-\\npetition, and other applicable laws. The authors and editor of this work\\nhave added value to the underlying factual material herein through one'),\n",
       " Document(metadata={'source': 'data/Medical_book.pdf', 'similarity_score': 0.253164291}, page_content='Christine O’Bryan,Graphic Specialist\\nMaria Franklin, Permissions Manager\\nMargaret A. Chamberlain, Permissions Specialist\\nMichelle DiMercurio, Senior Art Director\\nMike Logusz, Graphic Artist\\nMary Beth Trimper,Manager, Composition and\\nElectronic Prepress\\nEvi Seoud, Assistant Manager, Composition Purchasing\\nand Electronic Prepress\\nDorothy Maki, Manufacturing Manager\\nWendy Blurton, Senior Manufacturing Specialist\\nThe GALE\\nENCYCLOPEDIA\\nof MEDICINE\\nSECOND EDITION\\nSince this page cannot legibly accommodate all copyright notices, the\\nacknowledgments constitute an extension of the copyright notice.\\nWhile every effort has been made to ensure the reliability of the infor-\\nmation presented in this publication, the Gale Group neither guarantees\\nthe accuracy of the data contained herein nor assumes any responsibili-\\nty for errors, omissions or discrepancies. The Gale Group accepts no\\npayment for listing, and inclusion in the publication of any organiza-')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851883a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For synchronous calls, we already have sync_retriever from PineconeDB class\n",
    "sync_response = sync_retriever.invoke(\"what is acne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8094c02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='05b4dceb-33ab-4aca-8e0d-86398d62862c', metadata={'source': 'data/Medical_book.pdf'}, page_content='The goal of treating moderate acne is to decrease\\ninflammation and prevent new comedone formation. One\\neffective treatment is topical tretinoin along with a topical\\nGALE ENCYCLOPEDIA OF MEDICINE 2 25\\nAcne\\nAcne vulgaris affecting a woman’s face. Acne is the general\\nname given to a skin disorder in which the sebaceous\\nglands become inflamed. (Photograph by Biophoto Associ-\\nates, Photo Researchers, Inc. Reproduced by permission.)\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 25'),\n",
       " Document(id='c48a9784-043a-49c8-8492-5ea87845ea8d', metadata={'source': 'data/Medical_book.pdf'}, page_content='Description\\nAcne vulgaris, the medical term for common acne, is\\nthe most common skin disease. It affects nearly 17 million\\npeople in the United States. While acne can arise at any\\nage, it usually begins at puberty and worsens during ado-\\nlescence. Nearly 85% of people develop acne at some time\\nbetween the ages of 12-25 years. Up to 20% of women\\ndevelop mild acne. It is also found in some newborns.\\nThe sebaceous glands lie just beneath the skin’s sur-\\nface. They produce an oil called sebum, the skin’s natural\\nmoisturizer. These glands and the hair follicles within\\nwhich they are found are called sebaceous follicles.\\nThese follicles open onto the skin through pores. At\\npuberty, increased levels of androgens (male hormones)\\ncause the glands to produce too much sebum. When\\nexcess sebum combines with dead, sticky skin cells, a\\nhard plug, or comedo, forms that blocks the pore. Mild\\nnoninflammatory acne consists of the two types of come-\\ndones, whiteheads and blackheads.'),\n",
       " Document(id='596a63c4-8639-46c5-a339-f4a85b455572', metadata={'source': 'data/Medical_book.pdf'}, page_content='tranquilizers, antidepressants, antibiotics, oral contra-\\nceptives, and anabolic steroids.\\n• Personal hygiene. Abrasive soaps, hard scrubbing, or\\npicking at pimples will make them worse.\\n• Cosmetics. Oil-based makeup and hair sprays worsen\\nacne.\\n• Environment. Exposure to oils and greases, polluted air,\\nand sweating in hot weather aggravate acne.\\n• Stress. Emotional stress may contribute to acne.\\nAcne is usually not conspicuous, although inflamed\\nlesions may cause pain, tenderness, itching, or swelling.\\nThe most troubling aspects of these lesions are the nega-\\ntive cosmetic effects and potential for scarring. Some\\npeople, especially teenagers, become emotionally upset\\nabout their condition, and have problems forming rela-\\ntionships or keeping jobs.\\nDiagnosis\\nAcne patients are often treated by family doctors.\\nComplicated cases are referred to a dermatologist, a skin\\nGALE ENCYCLOPEDIA OF MEDICINE 224\\nAcne\\nGEM - 0001 to 0432 - A  10/22/03 1:41 PM  Page 24')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sync_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3deea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful AI medical assistant that helps people find information.\n",
    "You are given the following extracted parts of a long document and a question. Provide a conversational answer\n",
    "based on the context provided.\n",
    "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "Always include relevant sources in your answer.\n",
    "Make sure the answer is in detail and more than 100 words.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "If the question is out of context, politely inform them that you are tuned to only answer questions that are related to the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7a786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt),\n",
    "                (\"user\", \"{input}\")\n",
    "            ]     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf349421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "async_chain = (\n",
    "    {\n",
    "        \"context\": retriever,          # Pinecone search → returns docs\n",
    "        \"input\": RunnablePassthrough()  # pass the raw user query\n",
    "    }\n",
    "    | prompt\n",
    "    | llm \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72a1fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_chain = (\n",
    "    {\n",
    "        \"context\": sync_retriever,          # Pinecone search → returns docs\n",
    "        \"input\": RunnablePassthrough()  # pass the raw user query\n",
    "    }\n",
    "    | prompt\n",
    "    | llm  # ChatOpenAI(streaming=True) etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "59313bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def stream_answer(query):\n",
    "    # Keep the client alive during streaming\n",
    "    async for token in async_chain.astream(query):\n",
    "        print(token.content, end=\"\", flush=True)  # live streaming in notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "301cc4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 17:20:51,842|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n",
      "Based on the provided context, the project was built by Kirti Pogra using LangChain, Pinecone, and Groq. The code for this project is available on GitHub, and it is for educational purposes only."
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "await stream_answer(\"who build this project\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1ab5bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 17:21:02,003|(INFO)| File: _client | Message: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"]\n",
      "Based on the provided context, it appears that the project was built by Kirti Pogra. This information is mentioned in the first document (id='602d3e94-ceaa-46db-847e-c6cbf92d42df') which states: \"This project is built by Kirti Pogra using LangChain, Pinecone and Groq.\"\n",
      "\n",
      "The project is also mentioned to be available on GitHub, but no further information is provided about the project itself. \n",
      "\n",
      "Source: Document(id='602d3e94-ceaa-46db-847e-c6cbf92d42df')"
     ]
    }
   ],
   "source": [
    "for token in sync_chain.stream(\"who build this project?\"):\n",
    "    print(token.content, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E2EMedicalChatBotWithRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
